{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sparkOptimizedSVM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5VlLND-yxHK"
      },
      "source": [
        "# Run below commands\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yBX38LrzLDO"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4zFskoPzMwA",
        "outputId": "ffccb392-2eb7-4e1d-8d8a-c0ea78c7c064"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mvpCQ6LzTH4",
        "outputId": "8144f79c-017d-4b7f-800f-becc2b239c2d"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Test the spark \n",
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "\n",
        "df.show(3, False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uk0s27QbzpYZ",
        "outputId": "38cd0db1-bdd2-45c8-df13-3b1f9277ab12"
      },
      "source": [
        "# Check the pyspark version\n",
        "import pyspark\n",
        "print(pyspark.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHW4LdcMzuZp"
      },
      "source": [
        "import os\n",
        "import findspark\n",
        "import pandas as pd\n",
        "import pyspark\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "import pyspark.sql.types as T\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pyspark.ml.feature import StopWordsRemover \n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SQLContext\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "import pyspark.sql.types as T\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "\n",
        "from pyspark.ml.linalg import Vector\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import HashingTF,StopWordsRemover,IDF,Tokenizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "\n",
        "\n",
        "# import modules for feature transformation\n",
        "from pyspark.ml.linalg import Vector\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import HashingTF,StopWordsRemover,IDF,Tokenizer\n",
        "from pyspark.ml.feature import NGram\n",
        "from pyspark.ml.feature import RegexTokenizer\n",
        "#Tokenize into words\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcFQoZ5Ez3Me"
      },
      "source": [
        "TRAIN_FILE = \"/content/drive/MyDrive/CS651Final/jigsaw-toxic-comment-classification-challenge/train.csv\"\n",
        "TEST_DATA_FILE = \"/content/drive/MyDrive/CS651Final/jigsaw-toxic-comment-classification-challenge/test.csv\"\n",
        "TEST_LABEL = \"/content/drive/MyDrive/CS651Final/jigsaw-toxic-comment-classification-challenge/test_labels.csv\""
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mayYu01wOdIo"
      },
      "source": [
        "from pyspark.sql.types import ArrayType, StringType, IntegerType, StructType, StructField\n",
        "\n",
        "customSchema = StructType([\n",
        "  StructField(\"id\", StringType(), True),\n",
        "  StructField(\"comment_text\", StringType(), True),\n",
        "  StructField(\"toxic\", IntegerType(), True),\n",
        "  StructField(\"severe_toxic\", IntegerType(), True),\n",
        "  StructField(\"obscene\", IntegerType(), True),\n",
        "  StructField(\"threat\", IntegerType(), True),\n",
        "  StructField(\"insult\", IntegerType(), True),\n",
        "  StructField(\"identity_hate\", IntegerType(), True)]\n",
        ")"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMlXcrE_z7i1"
      },
      "source": [
        "# train = spark.read.csv(TRAIN_FILE, header='true', sep=',', multiLine=True, escape=\"\\\"\")\n",
        "train = spark.read.csv(TRAIN_FILE, header='true', multiLine=True, escape=\"\\\"\", schema=customSchema)\n",
        "test = spark.read.csv(TEST_DATA_FILE, header='true')"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKWVmxY0QyLs",
        "outputId": "0dd82a77-b81c-48b3-d2b4-a52ecf4aa882"
      },
      "source": [
        "train.count()"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "159571"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyHWz2l0Qb2V",
        "outputId": "2fd7c26f-545c-4173-b1b5-ba989a05880d"
      },
      "source": [
        "train.na.drop()"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: string, comment_text: string, toxic: int, severe_toxic: int, obscene: int, threat: int, insult: int, identity_hate: int]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI6t6gNeQ1Zm",
        "outputId": "f41fcbc9-5cdd-47bb-a807-bc39d8cf1c96"
      },
      "source": [
        "train.count()"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "159571"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qyc6jcDmQkDU",
        "outputId": "cf243154-e177-4253-de2f-13a550394b47"
      },
      "source": [
        "train.printSchema"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.printSchema of DataFrame[id: string, comment_text: string, toxic: int, severe_toxic: int, obscene: int, threat: int, insult: int, identity_hate: int]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYTbkvVv7C3C"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "train_clean = train.withColumn(\"comment_text\", regexp_replace(col(\"comment_text\"), \"[\\n\\r\\W]\", \" \"))\n",
        "train_clean = train_clean.withColumn(\"comment_text\", regexp_replace(col(\"comment_text\"), r'\\d+', \"\"))"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWRma99R1TRb"
      },
      "source": [
        "stemmer = SnowballStemmer(\"english\")\n",
        "def stemming(sentence):\n",
        "    stemSentence = \"\"\n",
        "    for word in sentence.split():\n",
        "        stem = stemmer.stem(word)\n",
        "        stemSentence += stem\n",
        "        stemSentence += \" \"\n",
        "    stemSentence = stemSentence.strip()\n",
        "    return stemSentence\n",
        "\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "# Stem text\n",
        "stemmer_udf = udf(lambda line: stemming(line), StringType())\n",
        "df_stemmed = train_clean.withColumn(\"comment_text\", stemmer_udf(\"comment_text\"))"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx1N8AdGGbFI",
        "outputId": "cd2a135d-83c6-46fa-d9eb-2c0fe944b3c4"
      },
      "source": [
        "df_stemmed.show()"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+--------------------+-----+------------+-------+------+------+-------------+\n",
            "|              id|        comment_text|toxic|severe_toxic|obscene|threat|insult|identity_hate|\n",
            "+----------------+--------------------+-----+------------+-------+------+------+-------------+\n",
            "|0000997932d777bf|explan whi the ed...|    0|           0|      0|     0|     0|            0|\n",
            "|000103f0d9cfb60f|d aww he match th...|    0|           0|      0|     0|     0|            0|\n",
            "|000113f07ec002fd|hey man i m reall...|    0|           0|      0|     0|     0|            0|\n",
            "|0001b41b1c6bb37e|more i can t make...|    0|           0|      0|     0|     0|            0|\n",
            "|0001d958c54c6e35|you sir are my he...|    0|           0|      0|     0|     0|            0|\n",
            "|00025465d4725e87|congratul from me...|    0|           0|      0|     0|     0|            0|\n",
            "|0002bcb3da6cb337|cocksuck befor yo...|    1|           1|      1|     0|     1|            0|\n",
            "|00031b1e95af7921|your vandal to th...|    0|           0|      0|     0|     0|            0|\n",
            "|00037261f536c51d|sorri if the word...|    0|           0|      0|     0|     0|            0|\n",
            "|00040093b2687caa|align on this sub...|    0|           0|      0|     0|     0|            0|\n",
            "|0005300084f90edc|fair use rational...|    0|           0|      0|     0|     0|            0|\n",
            "|00054a5e18b50dd4|bbq be a man and ...|    0|           0|      0|     0|     0|            0|\n",
            "|0005c987bdfc9d4b|hey what is it ta...|    1|           0|      0|     0|     0|            0|\n",
            "|0006f16e4e9f292e|befor you start t...|    0|           0|      0|     0|     0|            0|\n",
            "|00070ef96486d6f9|oh and the girl a...|    0|           0|      0|     0|     0|            0|\n",
            "|00078f8ce7eb276d|juelz santana age...|    0|           0|      0|     0|     0|            0|\n",
            "|0007e25b2121310b|bye don t look co...|    1|           0|      0|     0|     0|            0|\n",
            "|000897889268bc93|redirect talk voy...|    0|           0|      0|     0|     0|            0|\n",
            "|0009801bd85e5806|the mitsurugi poi...|    0|           0|      0|     0|     0|            0|\n",
            "|0009eaea3325de8c|don t mean to bot...|    0|           0|      0|     0|     0|            0|\n",
            "+----------------+--------------------+-----+------------+-------+------+------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAUbmxywMP2q"
      },
      "source": [
        "def check_clean(toxic, severe_toxic, obscene, threat, insult, identity_hate):\n",
        "    if (toxic + severe_toxic + obscene + threat + insult + identity_hate) > 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVrruZsjM-NP"
      },
      "source": [
        "from pyspark.sql.types import IntegerType"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwFHbbKLLDTk"
      },
      "source": [
        "mergeCols = udf(lambda toxic, severe_toxic, obscene, threat, insult, identity_hate: check_clean(toxic, severe_toxic, obscene, threat, insult, identity_hate), IntegerType())"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUvN7enULXy9"
      },
      "source": [
        "df_clean = df_stemmed.withColumn(\"clean\", mergeCols(df_stemmed[\"toxic\"], df_stemmed[\"severe_toxic\"], df_stemmed[\"obscene\"], df_stemmed[\"threat\"], df_stemmed[\"insult\"], df_stemmed[\"identity_hate\"]))"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSg8YxMgNOkr",
        "outputId": "b0378c12-47a7-49ad-d2b3-35d2c19a20f0"
      },
      "source": [
        "df_clean.show()"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+--------------------+-----+------------+-------+------+------+-------------+-----+\n",
            "|              id|        comment_text|toxic|severe_toxic|obscene|threat|insult|identity_hate|clean|\n",
            "+----------------+--------------------+-----+------------+-------+------+------+-------------+-----+\n",
            "|0000997932d777bf|explan whi the ed...|    0|           0|      0|     0|     0|            0|    1|\n",
            "|000103f0d9cfb60f|d aww he match th...|    0|           0|      0|     0|     0|            0|    1|\n",
            "|000113f07ec002fd|hey man i m reall...|    0|           0|      0|     0|     0|            0|    1|\n",
            "|0001b41b1c6bb37e|more i can t make...|    0|           0|      0|     0|     0|            0|    1|\n",
            "|0001d958c54c6e35|you sir are my he...|    0|           0|      0|     0|     0|            0|    1|\n",
            "|00025465d4725e87|congratul from me...|    0|           0|      0|     0|     0|            0|    1|\n",
            "|0002bcb3da6cb337|cocksuck befor yo...|    1|           1|      1|     0|     1|            0|    0|\n",
            "|00031b1e95af7921|your vandal to th...|    0|           0|      0|     0|     0|            0|    1|\n",
            "|00037261f536c51d|sorri if the word...|    0|           0|      0|     0|     0|            0|    1|\n",
            "|00040093b2687caa|align on this sub...|    0|           0|      0|     0|     0|            0|    1|\n",
            "|0005300084f90edc|fair use rational...|    0|           0|      0|     0|     0|            0|    1|\n",
            "|00054a5e18b50dd4|bbq be a man and ...|    0|           0|      0|     0|     0|            0|    1|\n",
            "|0005c987bdfc9d4b|hey what is it ta...|    1|           0|      0|     0|     0|            0|    0|\n",
            "|0006f16e4e9f292e|befor you start t...|    0|           0|      0|     0|     0|            0|    1|\n",
            "|00070ef96486d6f9|oh and the girl a...|    0|           0|      0|     0|     0|            0|    1|\n",
            "|00078f8ce7eb276d|juelz santana age...|    0|           0|      0|     0|     0|            0|    1|\n",
            "|0007e25b2121310b|bye don t look co...|    1|           0|      0|     0|     0|            0|    0|\n",
            "|000897889268bc93|redirect talk voy...|    0|           0|      0|     0|     0|            0|    1|\n",
            "|0009801bd85e5806|the mitsurugi poi...|    0|           0|      0|     0|     0|            0|    1|\n",
            "|0009eaea3325de8c|don t mean to bot...|    0|           0|      0|     0|     0|            0|    1|\n",
            "+----------------+--------------------+-----+------------+-------+------+------+-------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCyE8PCbRsIW"
      },
      "source": [
        "tokenizer = Tokenizer().setInputCol(\"comment_text\").setOutputCol(\"words\")\n",
        "#Remove stopwords\n",
        "remover= StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\").setCaseSensitive(False)\n",
        "# ngram = NGram().setN(2).setInputCol(\"filtered\").setOutputCol(\"ngrams\")\n",
        "#For each sentence (bag of words),use HashingTF to hash the sentence into a feature vector. \n",
        "hashingTF = HashingTF().setNumFeatures(1000).setInputCol(\"filtered\").setOutputCol(\"rawFeatures\")\n",
        "#Create TF_IDF features\n",
        "idf = IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\").setMinDocFreq(0)\n",
        "# Create a Logistic regression model\n",
        "lr = LinearSVC(labelCol=\"label\", featuresCol=\"features\", maxIter=20)\n",
        "# Streamline all above steps into a pipeline\n",
        "pipeline=Pipeline(stages=[tokenizer,remover,hashingTF,idf, lr])"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEnk043TSS1d"
      },
      "source": [
        "df_clean = df_clean.drop('toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate')"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keiodD5GXtII"
      },
      "source": [
        "df_clean = df_clean.withColumnRenamed(\"clean\", \"label\")"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8BEmyg-SmAx",
        "outputId": "0040933a-4130-4dde-f7bc-87454dfa3929"
      },
      "source": [
        "df_clean.show()"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+--------------------+-----+\n",
            "|              id|        comment_text|label|\n",
            "+----------------+--------------------+-----+\n",
            "|0000997932d777bf|explan whi the ed...|    1|\n",
            "|000103f0d9cfb60f|d aww he match th...|    1|\n",
            "|000113f07ec002fd|hey man i m reall...|    1|\n",
            "|0001b41b1c6bb37e|more i can t make...|    1|\n",
            "|0001d958c54c6e35|you sir are my he...|    1|\n",
            "|00025465d4725e87|congratul from me...|    1|\n",
            "|0002bcb3da6cb337|cocksuck befor yo...|    0|\n",
            "|00031b1e95af7921|your vandal to th...|    1|\n",
            "|00037261f536c51d|sorri if the word...|    1|\n",
            "|00040093b2687caa|align on this sub...|    1|\n",
            "|0005300084f90edc|fair use rational...|    1|\n",
            "|00054a5e18b50dd4|bbq be a man and ...|    1|\n",
            "|0005c987bdfc9d4b|hey what is it ta...|    0|\n",
            "|0006f16e4e9f292e|befor you start t...|    1|\n",
            "|00070ef96486d6f9|oh and the girl a...|    1|\n",
            "|00078f8ce7eb276d|juelz santana age...|    1|\n",
            "|0007e25b2121310b|bye don t look co...|    0|\n",
            "|000897889268bc93|redirect talk voy...|    1|\n",
            "|0009801bd85e5806|the mitsurugi poi...|    1|\n",
            "|0009eaea3325de8c|don t mean to bot...|    1|\n",
            "+----------------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ4BAWOcRznR"
      },
      "source": [
        "training_spark_df_binary, testing_spark_df_binary = df_clean.randomSplit([0.8, 0.2], seed = 2018)"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hkYV1kjS6Tc"
      },
      "source": [
        "paramGrid = ParamGridBuilder()\\\n",
        "    .addGrid(hashingTF.numFeatures,[1000]) \\\n",
        "    .addGrid(lr.regParam, [0.1]) \\\n",
        "    .build()"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRKAtjpSTrx_"
      },
      "source": [
        "crossval = TrainValidationSplit(estimator=pipeline,\n",
        "                            estimatorParamMaps=paramGrid,\n",
        "                            evaluator=BinaryClassificationEvaluator().setMetricName('areaUnderPR'), # set area Under precision-recall curve as the evaluation metric\n",
        "                            # 80% of the data will be used for training, 20% for validation.\n",
        "                            trainRatio=0.8)"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7Xig15oT29X"
      },
      "source": [
        "cvModel = crossval.fit(training_spark_df_binary)"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTQvZ8e4aMie"
      },
      "source": [
        "train_prediction = cvModel.transform(training_spark_df_binary)\n",
        "test_prediction = cvModel.transform(testing_spark_df_binary)"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYYCeIUnaxDj",
        "outputId": "1d4ac4e6-18cb-40f8-e625-bea245eab28e"
      },
      "source": [
        "pd_prediction = test_prediction.select(\"*\").toPandas()\n",
        "actual = pd_prediction[\"label\"].tolist()\n",
        "pred = pd_prediction[\"prediction\"].tolist()\n",
        "\n",
        "# pd_prediction_other_dataset = otherDatasetTest.select(\"*\").toPandas()\n",
        "# actual_otherdataset = pd_prediction_other_dataset[\"label\"].tolist()\n",
        "# pred_otherdataset = pd_prediction_other_dataset[\"prediction\"].tolist()\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(actual, pred).ravel()\n",
        "print(confusion_matrix(actual, pred))\n",
        "print(\"true positive rate\",tp / (tp + fp))\n",
        "print(\"true negative rate\",tn / (tn + fp))\n",
        "\n",
        "# compute the accuracy score on training data\n",
        "correct_train = train_prediction.filter(train_prediction.label == train_prediction.prediction).count()  \n",
        "accuracy_train = correct_train/train_prediction.count() # store the training accuracy score\n",
        "print('Training set accuracy {:.2%} data items: {}, correct: {}'.format(accuracy_train,train_prediction.count(), correct_train))\n",
        "    \n",
        "# Caculate the accuracy score for the best model \n",
        "correct_test = test_prediction.filter(test_prediction.label == test_prediction.prediction).count()  \n",
        "accuracy_test = correct_test/test_prediction.count()\n",
        "print('Testing set accuracy {:.2%} data items: {}, correct: {}'.format(accuracy_test, test_prediction.count(), correct_test))\n",
        "    \n",
        "# Caculate the accuracy score for the best model \n",
        "# correct_test_otherDataset = otherDatasetTest.filter(otherDatasetTest.label == otherDatasetTest.prediction).count()  \n",
        "# accuracy_test_otherDataset = correct_test_otherDataset/otherDatasetTest.count()\n",
        "# print('Testing set accuracy for other dataset is  {:.2%} data items: {}, correct: {}'.format(accuracy_test_otherDataset, otherDatasetTest.count(), correct_test_otherDataset))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   50  3170]\n",
            " [    4 28675]]\n",
            "true positive rate 0.900455330507144\n",
            "true negative rate 0.015527950310559006\n",
            "Training set accuracy 90.00% data items: 127672, correct: 114908\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}